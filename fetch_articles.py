import feedparser
import os
import re
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from time import mktime

from dotenv import load_dotenv

load_dotenv()

RSS_FEEDS = {
    "Lenny's Newsletter": "https://www.lennysnewsletter.com/feed",
    "SVPG": "https://www.svpg.com/feed/",
    "Intercom Blog": "https://www.intercom.com/blog/feed/",
    "Andrew Chen": "https://andrewchen.substack.com/feed",
}

DAYS = 1
MONTHS = {
    "JAN": 1, "FEB": 2, "MAR": 3, "APR": 4, "MAY": 5, "JUN": 6,
    "JUL": 7, "AUG": 8, "SEP": 9, "OCT": 10, "NOV": 11, "DEC": 12,
}


def fetch_recent_articles(feed_url, days=DAYS):
    """Parse RSS feed and return articles from the last `days` days."""
    feed = feedparser.parse(feed_url)
    cutoff = datetime.now(timezone.utc) - timedelta(days=days)
    articles = []
    for entry in feed.entries:
        published = datetime.fromtimestamp(mktime(entry.published_parsed), tz=timezone.utc)
        if published >= cutoff:
            articles.append({
                "title": entry.title,
                "link": entry.link,
                "date": published.strftime("%Y-%m-%d"),
            })
    return articles


def scrape_mindtheproduct(days=DAYS):
    """Scrape Mind the Product homepage for recent articles."""
    resp = requests.get(
        "https://www.mindtheproduct.com",
        headers={"User-Agent": "Mozilla/5.0"},
        timeout=15,
    )
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    cutoff = datetime.now(timezone.utc) - timedelta(days=days)
    today = datetime.now(timezone.utc)
    date_pattern = re.compile(
        r"^(JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|OCT|NOV|DEC)\s+\d{1,2}$"
    )

    seen = set()
    articles = []
    for div in soup.find_all("div", string=date_pattern):
        date_text = div.get_text(strip=True)
        match = re.match(r"(\w+)\s+(\d+)", date_text)
        if not match:
            continue
        month = MONTHS[match.group(1)]
        day = int(match.group(2))
        year = today.year
        try:
            pub_date = datetime(year, month, day, tzinfo=timezone.utc)
        except ValueError:
            continue
        if pub_date > today:
            pub_date = pub_date.replace(year=year - 1)
        if pub_date < cutoff:
            continue

        parent = div
        for _ in range(10):
            parent = parent.parent
            if parent is None:
                break
            link = parent.find("a", href=re.compile(r"^/[a-z]"))
            if link:
                href = link["href"]
                if href in seen:
                    break
                seen.add(href)
                all_text = link.get_text(separator="|", strip=True).split("|")
                candidates = [
                    t for t in all_text
                    if len(t) > 20 and not date_pattern.match(t)
                ]
                title = candidates[0] if candidates else link.get_text(strip=True)
                articles.append({
                    "title": title,
                    "link": "https://www.mindtheproduct.com" + href,
                    "date": pub_date.strftime("%Y-%m-%d"),
                })
                break

    return articles


# --- AI æ‘˜è¦ ---

def fetch_article_content(url):
    """æŠ“å–æ–‡ç« ç¶²é æ­£æ–‡ã€‚"""
    try:
        resp = requests.get(
            url,
            headers={"User-Agent": "Mozilla/5.0"},
            timeout=15,
        )
        resp.raise_for_status()
    except requests.RequestException:
        return None

    soup = BeautifulSoup(resp.text, "html.parser")

    # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
    for tag in soup.find_all(["script", "style", "nav", "footer", "header"]):
        tag.decompose()

    # å˜—è©¦æ‰¾ article æ¨™ç±¤ï¼Œå¦å‰‡ç”¨ body
    article = soup.find("article") or soup.find("body")
    if not article:
        return None

    text = article.get_text(separator="\n", strip=True)
    # æˆªæ–·éé•·çš„å…§å®¹ï¼ˆç¯€çœ API tokenï¼‰
    return text[:5000] if text else None


def summarize_article(title, content):
    """ç”¨ Claude API ç”¢ç”Ÿæ–‡ç« æ‘˜è¦ã€‚"""
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key:
        return None

    import anthropic
    client = anthropic.Anthropic(api_key=api_key)

    try:
        message = client.messages.create(
            model="claude-sonnet-4-5-20250929",
            max_tokens=300,
            messages=[{
                "role": "user",
                "content": (
                    f"è«‹ç”¨ç¹é«”ä¸­æ–‡ç‚ºä»¥ä¸‹æ–‡ç« å¯« 3-5 å¥æ‘˜è¦ï¼Œé‡é»æ‘˜è¿°æ–‡ç« çš„æ ¸å¿ƒè§€é»å’Œé—œéµæ´è¦‹ã€‚\n\n"
                    f"æ–‡ç« æ¨™é¡Œï¼š{title}\n\n"
                    f"æ–‡ç« å…§å®¹ï¼š\n{content}"
                ),
            }],
        )
        return message.content[0].text
    except Exception as e:
        print(f"    AI æ‘˜è¦å¤±æ•—: {e}")
        return None


def summarize_all_articles(all_articles):
    """å°æ‰€æœ‰æ–‡ç« ç”¢ç”Ÿ AI æ‘˜è¦ã€‚"""
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key:
        print("  æœªè¨­å®š ANTHROPIC_API_KEYï¼Œè·³é AI æ‘˜è¦")
        return

    total = sum(len(arts) for arts in all_articles.values())
    if total == 0:
        return

    print(f"\nç”¢ç”Ÿ AI æ‘˜è¦ï¼ˆå…± {total} ç¯‡ï¼‰...")
    count = 0
    for source, articles in all_articles.items():
        for article in articles:
            count += 1
            print(f"  [{count}/{total}] {article['title'][:50]}...")
            content = fetch_article_content(article["link"])
            if content:
                summary = summarize_article(article["title"], content)
                article["summary"] = summary
            else:
                article["summary"] = None


# --- Markdown ç”¢å‡º ---

def build_markdown(all_articles, today):
    """Build markdown string from collected articles."""
    lines = [f"# æ–‡ç« æ‘˜è¦ - {today}", ""]
    for source, articles in all_articles.items():
        lines.append(f"## {source}")
        if not articles:
            lines.append("æœ€è¿‘ 24 å°æ™‚æ²’æœ‰æ–°æ–‡ç« ã€‚")
            lines.append("")
            continue
        lines.append("| æ—¥æœŸ | æ¨™é¡Œ |")
        lines.append("|------|------|")
        for a in articles:
            lines.append(f"| {a['date']} | [{a['title']}]({a['link']}) |")
        lines.append("")
        # åŠ ä¸Šæ¯ç¯‡æ–‡ç« çš„æ‘˜è¦
        for a in articles:
            if a.get("summary"):
                lines.append(f"**{a['title']}**")
                lines.append(f"{a['summary']}")
                lines.append("")
    return "\n".join(lines)


# --- LINE é€šçŸ¥ ---

def build_line_message(all_articles, today):
    """å°‡æ‘˜è¦è½‰ç‚º LINE é©åˆçš„ç´”æ–‡å­—æ ¼å¼ã€‚"""
    lines = [f"ğŸ“° æ–‡ç« æ‘˜è¦ - {today}", ""]
    has_articles = False
    for source, articles in all_articles.items():
        if not articles:
            continue
        has_articles = True
        lines.append(f"ã€{source}ã€‘")
        for a in articles:
            lines.append(f"ğŸ“Œ {a['title']}")
            if a.get("summary"):
                lines.append(a["summary"])
            lines.append(f"ğŸ”— {a['link']}")
            lines.append("")
        lines.append("---")
    if not has_articles:
        lines.append("ä»Šå¤©æ²’æœ‰æ–°æ–‡ç« ã€‚")
    return "\n".join(lines)


def send_line_message(text):
    """ç™¼é€è¨Šæ¯åˆ° LINE ç¾¤çµ„ã€‚"""
    token = os.getenv("LINE_CHANNEL_ACCESS_TOKEN")
    group_id = os.getenv("LINE_GROUP_ID")
    if not token or not group_id:
        print("  æœªè¨­å®š LINE_CHANNEL_ACCESS_TOKEN æˆ– LINE_GROUP_IDï¼Œè·³é LINE é€šçŸ¥")
        return False

    # LINE è¨Šæ¯é™åˆ¶ 5000 å­—å…ƒ
    if len(text) > 5000:
        text = text[:4990] + "\n..."

    resp = requests.post(
        "https://api.line.me/v2/bot/message/push",
        headers={
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json",
        },
        json={
            "to": group_id,
            "messages": [{"type": "text", "text": text}],
        },
        timeout=15,
    )
    if resp.status_code == 200:
        print("  LINE è¨Šæ¯ç™¼é€æˆåŠŸ")
        return True
    else:
        print(f"  LINE ç™¼é€å¤±æ•—: {resp.status_code} {resp.text}")
        return False


# --- å‰ç½®æª¢æŸ¥ ---

def run_preflight_checks():
    """åŸ·è¡Œå‰ç½®æª¢æŸ¥ï¼Œç¢ºèªæ‰€æœ‰ä¾†æºå¯æ­£å¸¸æŠ“å–ã€‚"""
    import subprocess
    print("åŸ·è¡Œå‰ç½®æª¢æŸ¥ ...")
    result = subprocess.run(
        ["python3", "test_fetch.py"],
        capture_output=True, text=True,
    )
    print(result.stdout)
    if result.returncode != 0:
        print(result.stderr)
        print("å‰ç½®æª¢æŸ¥å¤±æ•—ï¼Œä¸­æ­¢åŸ·è¡Œã€‚")
        raise SystemExit(1)
    print()


# --- ä¸»ç¨‹å¼ ---

def main():
    run_preflight_checks()

    today = datetime.now().strftime("%Y-%m-%d")
    all_articles = {}

    # RSS sources
    for source, url in RSS_FEEDS.items():
        print(f"æŠ“å– {source} ...")
        try:
            articles = fetch_recent_articles(url)
        except Exception as e:
            print(f"  æŠ“å–å¤±æ•—ï¼Œè·³é: {e}")
            articles = []
        all_articles[source] = articles
        print(f"  æ‰¾åˆ° {len(articles)} ç¯‡æœ€è¿‘ {DAYS} å¤©çš„æ–‡ç« ")

    # Scrape sources
    print("æŠ“å– Mind the Product ...")
    try:
        articles = scrape_mindtheproduct()
    except Exception as e:
        print(f"  æŠ“å–å¤±æ•—ï¼Œè·³é: {e}")
        articles = []
    all_articles["Mind the Product"] = articles
    print(f"  æ‰¾åˆ° {len(articles)} ç¯‡æœ€è¿‘ {DAYS} å¤©çš„æ–‡ç« ")

    # AI æ‘˜è¦
    summarize_all_articles(all_articles)

    # Markdown ç”¢å‡º
    md = build_markdown(all_articles, today)
    print("\n" + md)

    os.makedirs("output", exist_ok=True)
    output_path = os.path.join("output", f"digest_{today}.md")
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(md)
    print(f"\nå·²å¯«å…¥ {output_path}")

    # LINE é€šçŸ¥
    print("\nç™¼é€ LINE é€šçŸ¥ ...")
    line_msg = build_line_message(all_articles, today)
    send_line_message(line_msg)


if __name__ == "__main__":
    main()
